{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant_Vision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NCn5b0HjEuuYobxS6Zy42GttHs4VZ-xw",
      "authorship_tag": "ABX9TyOyjRpHmwTmRcQ0M9V0gRG1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPstartDeveloper/DS-2.4-Advanced-Topics/blob/main/Notebooks/Computer_Vision/Plant_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnfCwoEWRcvg"
      },
      "source": [
        "# Diagnosing Crop Leaves\n",
        "\n",
        "Source: in this notebook we will use the \"New Plant Diseases Dataset\", originally uploaded to [Kaggle](https://www.kaggle.com/vipoooool/new-plant-diseases-dataset) by Samir Bhattarai.\n",
        "\n",
        "\n",
        "**MY PROCESS**: I started with the [instructions from video by Krish Naik](https://youtu.be/chQNuV9B-Rw) to do transfer learning with Inception V3, then went ahead with the following adjustments:\n",
        "\n",
        "1. There was an imbalance in the number of labels represented in the `test/` data directory that originally came with the dataset. So instead I combined it all into the `train/` directory, and then split the data into training and testing sections using `ImageDataGenerator`.\n",
        "2. The original notebook used Inception V3 - I decided against using transfer learning to start out with, so I could learn a little more about building a *Convolutional Neural Network* (CNN) in the beginning versions of this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBa4mcG8rq5b"
      },
      "source": [
        "## Let's Train on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUJuzF5hq_8v",
        "outputId": "6e304308-595b-4a46-de82-379f4c22b1c5"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/6d/67169e8d8146f377bbfd71d6c108a0fce218411371ce41d440a7a5f5fb20/tensorflow_gpu-2.4.1-cp36-cp36m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.4.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (51.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iZujrqqsvt4"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-cFWqPyrvKH"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "# Pre-trained Models We May Use for Transfer learning\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "# Preprocessing Data \n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "# Keras Callbacks\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, \n",
        "    ModelCheckpoint,\n",
        "    TensorBoard,\n",
        ")\n",
        "# ease of handling data, and visualizing data\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "# Layers for the Model\n",
        "from keras.layers import (\n",
        "    Dense,\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    BatchNormalization\n",
        ")\n",
        "%matplotlib inline"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "142D-HTwq2em"
      },
      "source": [
        "## Get the Data \n",
        "The file paths to the data are below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRdg4GmPRZQw"
      },
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/Colab-Notebooks/DS-2.4/Notebooks/Computer_Vision/new-plant-diseases-dataset'\n",
        "TRAIN_PATH = f\"{DATA_PATH}/train\"\n",
        "VAL_PATH = f\"{DATA_PATH}/valid\"\n",
        "# The test directory was empty on my machine\n",
        "# TEST_PATH = f\"{DATA_PATH}/test\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss589OemwZG4"
      },
      "source": [
        "## How Many Classes Are in the Dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytSno_uTwced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ffd238-706a-4055-aa48-bf05048a5bae"
      },
      "source": [
        "# get the number of output classes\n",
        "NUM_CLASSES = len(glob(f\"{TRAIN_PATH}/*\"))  # 38\n",
        "print(f\"There are {NUM_CLASSES} classes in the dataset.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 38 classes in the dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luo1IaPcumKM"
      },
      "source": [
        "## What Are the Dimensions of the Dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vhET53XukeE"
      },
      "source": [
        "# desired image dimensions\n",
        "IMAGE_SIZE = [224, 224, 3]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hwG8hRnLe0d"
      },
      "source": [
        "## Examples of the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5UBBAiiLiKi"
      },
      "source": [
        "# TODO: print examples using imshow"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmdtR9tvkzZ"
      },
      "source": [
        "# Defining the CNN Architecture\n",
        "This model is an example of a CNN (*convolutional neural network*). \n",
        "\n",
        "We will be using the Sequential API in this implementation. The architecture is summarized below:\n",
        "\n",
        "- 3 Convolutional Layers, 4 neurons each\n",
        "- 1 Dense layer\n",
        "- Optimizer Algorithm is Adam\n",
        "- Drop Out Rate is 20%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuBQ1rLwhBsh"
      },
      "source": [
        "### Utility Functions to Make Adding layers Easier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOWz-Pl9gn6W"
      },
      "source": [
        "def add_conv_layer(model, layer_size, needs_input, input_shape, kernel_size=None, pool_size=None):\n",
        "    \"\"\"Add a Keras convolutional layer to the model, along with MaxPooling.\n",
        "       Will specify input shape as well if needed.\n",
        "       \n",
        "       Parameters:\n",
        "       model(Model): Neural network in Keras\n",
        "       layer_size(int): number of neurons to go in layer\n",
        "       need_input(bool): signals if the convolutional layer needs to specify\n",
        "                         the dimensions of the input\n",
        "       input_shape(tuple or list): 1D vector of what the dataset dimensions are\n",
        "       kernel_size(tuple): specifies a square matrix to use for kernel dimensions\n",
        "       pool_size(tuple): specifies a square matrix to use in pooling\n",
        "       \n",
        "       Returns: None\n",
        "       \n",
        "    \"\"\"\n",
        "    # set kernel and pool size\n",
        "    if kernel_size is None:\n",
        "        kernel_size = (3, 3)\n",
        "    if pool_size is None:\n",
        "        pool_size = (2, 2)\n",
        "    # specify input dimension for 1st conv layer\n",
        "    if needs_input is True:\n",
        "        conv_layer = Conv2D(layer_size,\n",
        "                            kernel_size=kernel_size,\n",
        "                            activation='relu',\n",
        "                            input_shape=input_shape)\n",
        "\n",
        "    else:\n",
        "        # otherwise all other convolutional layers don't need it\n",
        "        conv_layer = Conv2D(layer_size,\n",
        "                            kernel_size=kernel_size,\n",
        "                            activation='relu')\n",
        "    # add Convolutional layer\n",
        "    model.add(conv_layer)  \n",
        "    # add MaxPooling layer\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))  # no learning params\n",
        "    return None\n",
        "                  \n",
        "    \n",
        "def add_dense_layer(model, layer_size, is_output, drop_rate):\n",
        "    \"\"\"Add a multi-layer perceptron to the model\n",
        "       Will specify 'softmax' for the final layer.\n",
        "       \n",
        "       Parameters:\n",
        "       model(Model): Neural network in Keras\n",
        "       layer_size(int): number of neurons to go in layer\n",
        "       is_output(bdool): signals if the MLP is the last layer\n",
        "       drop_rate(float): percentage of connections in Dense layer\n",
        "                       to cut off\n",
        "       \n",
        "       Returns: None\n",
        "       \n",
        "    \"\"\"\n",
        "    # specify activation function\n",
        "    activation = 'relu' if is_output is False else 'softmax'\n",
        "    # add MLP\n",
        "    model.add(Dense(layer_size, activation=activation)) \n",
        "    # Add Dropout layer and Batch Normalization\n",
        "    if is_output is False:\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(drop_rate))\n",
        "    return None\n",
        "\n",
        "\n",
        "def compile_model(model, optimizer=None):\n",
        "    \"\"\"Compile the neural network.\n",
        "    \n",
        "       Parameter:\n",
        "       model(keras.Sequential or keras.Model): the model object\n",
        "       optimizer(str): specfies the algoritm used to minimize loss\n",
        "       \n",
        "       Returns: None\n",
        "       \n",
        "    \"\"\"\n",
        "    # set the optimizer\n",
        "    if optimizer is None:\n",
        "        optimizer = 'adam'\n",
        "    # compile the model\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', optimizer=optimizer,\n",
        "        metrics=[\n",
        "          'accuracy', tf.keras.metrics.Precision(),\n",
        "          tf.keras.metrics.Recall()]\n",
        "    )\n",
        "    return None\n",
        "\n",
        "def define_model(units, conv_layers, dense_layers, dropout, input_shape):\n",
        "    \"\"\"Define a CNN + MLP model in Keras.\n",
        "    \n",
        "       Parameters:\n",
        "       units(int): number of neurons to go in a layer\n",
        "       conv_layers(int): number of convolutional layers\n",
        "       dense_layers(int): number of MLP\n",
        "       dropout(float): percentage of connections in the Dense layer(s) to cut \n",
        "        input_shape(tuple or list): 1D vector of what the dataset dimensions are\n",
        "                       \n",
        "       Returns: tf.keras.Sequential: the neural network to train\n",
        "    \n",
        "    \"\"\"\n",
        "    # Instaniate model\n",
        "    model = Sequential()\n",
        "    # Add CNN layers\n",
        "    add_conv_layer(model, units, True, input_shape)\n",
        "    for l in range(conv_layers - 1):\n",
        "        # add convolutional layers that come after the 1st\n",
        "        add_conv_layer(model, units, False, input_shape)\n",
        "    # Flatten the data\n",
        "    model.add(Flatten())\n",
        "    # Add MLP Layers\n",
        "    for l in range(dense_layers - 1):\n",
        "        add_dense_layer(model, units, False, dropout)\n",
        "    # add final MLP, for output\n",
        "    add_dense_layer(model, NUM_CLASSES, True, dropout)\n",
        "    # Compile Model\n",
        "    compile_model(model)\n",
        "    return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRJLOsl-iy3_"
      },
      "source": [
        "### Instantiation of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T-vdrt9tRXQ"
      },
      "source": [
        "# build the model\n",
        "conv_layers = 3\n",
        "dense_layers = 1\n",
        "layer_size = 4\n",
        "\n",
        "# Define Model\n",
        "model = define_model(layer_size, conv_layers, dense_layers, 0.2, IMAGE_SIZE)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcbwpvlqxjhS"
      },
      "source": [
        "## Summarize the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_R-WZufxRDK",
        "outputId": "9940c9e2-1c37-4a35-b4e8-b1a25b2503f3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 222, 222, 4)       112       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 111, 111, 4)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 109, 109, 4)       148       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 54, 54, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 52, 52, 4)         148       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 26, 26, 4)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2704)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 38)                102790    \n",
            "=================================================================\n",
            "Total params: 103,198\n",
            "Trainable params: 103,198\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3B5cqSv0Rvl"
      },
      "source": [
        "## Set the Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQq2IaCf0UTJ"
      },
      "source": [
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKjtkCpc0i9j"
      },
      "source": [
        "### Use Data Generators\n",
        "Due to the size of the dataset (87K images), this step will make the training process more efficient.\n",
        "\n",
        "We scale all the data, and data augmentation is only on the training data (to make the model more robust)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELJX4mkO03m2"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, shear_range=0.2,\n",
        "    zoom_range=0.2, horizontal_flip = True,\n",
        "    validation_split=0.2  # set aside some data for validation\n",
        "  )\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZAgeyAg1Kgj"
      },
      "source": [
        "### Load the Train, Test, and Validation Sets\n",
        "We scale all the data, and data augmentation is only on the training data (to make the model more robust)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPzwtILU1N2e",
        "outputId": "0b4b80e1-6b01-40c6-a076-9158abf7aa1e"
      },
      "source": [
        "# Make sure you provide the same target size as initialized for the image size\n",
        "image_width, image_height = IMAGE_SIZE[0], IMAGE_SIZE[1]\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    TRAIN_PATH, target_size=(image_width, image_height),\n",
        "    batch_size=80, class_mode='categorical',\n",
        "    subset='training'\n",
        "  )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 56316 images belonging to 38 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJTlGoSb5ZWL",
        "outputId": "1a803b13-de0b-4e5a-f621-81dd0d41eec0"
      },
      "source": [
        "# use the rest of the train/ dir for validation data\n",
        "image_width, image_height = IMAGE_SIZE[0], IMAGE_SIZE[1]\n",
        "val_set = train_datagen.flow_from_directory(\n",
        "    TRAIN_PATH, target_size=(image_width, image_height),\n",
        "    batch_size=80, class_mode='categorical',\n",
        "    subset='validation'\n",
        "  )"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14059 images belonging to 38 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C54QFig1g5t",
        "outputId": "d1eb2f51-a288-4b8f-8adb-c27a61f5690c"
      },
      "source": [
        "# use the valid/ dir for testing data\n",
        "image_width, image_height = IMAGE_SIZE[0], IMAGE_SIZE[1]\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    VAL_PATH, target_size=(image_width, image_height),\n",
        "    batch_size=80, class_mode='categorical' \n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17601 images belonging to 38 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGeoCWCCudhN"
      },
      "source": [
        "### Using Keras Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7JWLThouh00"
      },
      "source": [
        "# use TensorBoard to monitor training\n",
        "# name the model for TensorBoard\n",
        "NAME = (\n",
        "    f'{3}-conv-{4}-nodes' +\n",
        "    f'-{1}-dense_layers'\n",
        ")\n",
        "# Instantiate TensorBoard to visualize model performance\n",
        "tensorboard = TensorBoard(log_dir=f'./Graph/{NAME}')\n",
        "\n",
        "# using ModelCheckpoint, for convenience of coming back to train more later\n",
        "checkpoint_filepath = './tmp/checkpoint'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,  # we'll need to remake the architecture again\n",
        "    monitor='accuracy',\n",
        "    mode='max',\n",
        "    save_freq='epoch',  # save every epoch\n",
        "    save_best_only=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJEKyW4F5RAj"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CksGUbsN4lal",
        "outputId": "9b2f6239-ed2d-4584-9ac1-b3e8da7c0e32"
      },
      "source": [
        "while True:  # preventing runtime timeout\n",
        "  history = model.fit(\n",
        "      x=training_set,  # pass in the generator to determine x, y, and batch_size\n",
        "      epochs=3, validation_data=val_set,\n",
        "      steps_per_epoch=len(training_set),\n",
        "      validation_steps=len(val_set),\n",
        "      callbacks=[\n",
        "        tensorboard, model_checkpoint_callback\n",
        "      ]\n",
        "  )\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "704/704 [==============================] - ETA: 0s - loss: 2.1093 - accuracy: 0.4049"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5NxWF7ul3Ii"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TjZDpFyoNXz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "3a5359cc-9831-411d-cf2e-8daad44cc4b7"
      },
      "source": [
        "def plot_metric(history, metric):\n",
        "    \"\"\"Plots the model performance metric for training\n",
        "       testing data using Matplotlib.\n",
        "       \n",
        "       Parameters\n",
        "       history(History): contains data on model's metrics over the\n",
        "                         course of the training\n",
        "       metric(str): the name of the metric. Will be one of \n",
        "                    the keys in the History object, e.g. 'loss'\n",
        "       \n",
        "       Returns: None\n",
        "       \n",
        "    \"\"\"\n",
        "    # plot for training data\n",
        "    plt.plot(history.history[metric])\n",
        "    # plot for testing data\n",
        "    val_form = f'val_{metric}'\n",
        "    plt.plot(history.history[val_form])\n",
        "    # add meta-info about graph - title, labels, legend\n",
        "    plt.title(f'Model {metric}')\n",
        "    plt.ylabel(metric)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    # show the graph\n",
        "    plt.show()\n",
        "    return None\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f34dc1a403c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7D9qh3rmPer"
      },
      "source": [
        "# TODO: plot validation accuracy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcRpHoLZmUcc"
      },
      "source": [
        "# TODO plot validation loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDp-FMsImXBO"
      },
      "source": [
        "## Export the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv_ok7ORme5E"
      },
      "source": [
        "def save_model(model, weights_file, architecture_file):\n",
        "    \"\"\"Save the model weights and architecture.\n",
        "    \n",
        "       Parameters: \n",
        "       model(Model): keras Model object being saved\n",
        "       weights_file(str): name of the Hadoop file where\n",
        "                          weights will be saved\n",
        "       architecture_file(str): name of the JSON file where \n",
        "                               model architecture is to be\n",
        "                               saved\n",
        "                               \n",
        "       Returns: None\n",
        "       \n",
        "    \"\"\"\n",
        "    # Save the weights\n",
        "    model.save_weights(f'{weights_file}.h5')\n",
        "    # Save the architecture\n",
        "    with open(f'{architecture_file}.json', 'w') as f:\n",
        "        f.write(model.to_json())\n",
        "    return None\n",
        "\n",
        "# TODO: save the model architecture in JSON, and the weights in a Hadoop file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS2DgwMZmip2"
      },
      "source": [
        "## Final Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgI31GZImuFL"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Use [TensorBoard](https://www.ecosia.org/search?q=tensorboard&addon=chrome&addonversion=3.2.0&method=topbar) and [Hyperas](https://github.com/maxpumperla/hyperas/issues) to experiment with what the best architecture and hyperparameters to experiment with what the best hyperparameters are for the model.\n",
        "2. Check for class imbalances, and if there are try to upsample the minority classes (or adjust the class weights) in order to address the issue.\n",
        "3. Try using pre-trained weights from a popular CNN like ResNet or Inception, and see if we get a better performing model."
      ]
    }
  ]
}