{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stack_Overflow_text_classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBMu+pXjZtIItNoVcSj1xB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPstartDeveloper/DS-2.4-Advanced-Topics/blob/main/Notebooks/NLP/Stack_Overflow_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpU5yadWuNIK"
      },
      "source": [
        "# What Language is It? \n",
        "## Text Classification of Stack Overflow Questions, using Keras \n",
        "\n",
        "Originally adapted from the botton of [this tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb) on binary text classification from Tensorflow. The original notebook stated:\n",
        "\n",
        "> ### Exercise: multiclass classification on Stack Overflow questions\n",
        "\n",
        "> This tutorial showed how to train a binary classifier from scratch on the IMDB dataset. As an exercise, you can modify this notebook to train a multiclass classifier to predict the tag of a programming question on [Stack Overflow](http://stackoverflow.com/).\n",
        "\n",
        "> We have prepared a [dataset](http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz) for you to use containing the body of several thousand programming questions (for example, \"How can sort a dictionary by value in Python?\") posted to Stack Overflow. Each of these is labeled with exactly one tag (either Python, CSharp, JavaScript, or Java). Your task is to take a question as input, and predict the appropriate tag, in this case, Python. \n",
        "\n",
        "> The dataset you will work with contains several thousand questions extracted from the much larger public Stack Overflow dataset on [BigQuery](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow), which contains more than 17 million posts.\n",
        "\n",
        "> After downloading the dataset, you will find it has a similar directory structure to the IMDB dataset you worked with previously:\n",
        "\n",
        "> ```\n",
        "train/\n",
        "...python/\n",
        "......0.txt\n",
        "......1.txt\n",
        "...javascript/\n",
        "......0.txt\n",
        "......1.txt\n",
        "...csharp/\n",
        "......0.txt\n",
        "......1.txt\n",
        "...java/\n",
        "......0.txt\n",
        "......1.txt\n",
        "```\n",
        "\n",
        "> Note: to increase the difficulty of the classification problem, we have replaced any occurences of the words Python, CSharp, JavaScript, or Java in the programming questions with the word *blank* (as many questions contain the language they're about). \n",
        "\n",
        "> To complete this exercise, you should modify this notebook to work with the Stack Overflow dataset by making the following modifications:\n",
        "\n",
        "> 1. At the top of your notebook, update the code that downloads the IMDB dataset with code to download the [Stack Overflow dataset](http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz) we have prepreared. As the Stack Overflow dataset has a similar directory structure, you will not need to make many modifications. \n",
        "\n",
        "> 2. Modify the last layer of your model to read `Dense(4)`, as there are now four output classes.\n",
        "\n",
        "> 3. When you compile your model, change the loss to `losses.SparseCategoricalCrossentropy`. This is the correct loss function to use for a multiclass classification problem, when the labels for each class are integers (in our case, they can be 0, *1*, *2*, or *3*).\n",
        "\n",
        "> 4. Once these changes are complete, you will be able to train a multiclass classifier. \n",
        "\n",
        "> If you get stuck, you can find a solution [here](https://github.com/tensorflow/examples/blob/master/community/en/text_classification_solution.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uq9_MN0vcgW"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhOmDy11veyz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVqEhxORvked"
      },
      "source": [
        "## What Version of Tensorflow is This?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUeGYEZcvi2x",
        "outputId": "9fd8e3d4-2106-4536-f3cb-d638c737da89"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUQufujHvyMU"
      },
      "source": [
        "## Download and Explore the Stack Overflow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538ZU0DTv1MZ"
      },
      "source": [
        "url = \"http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
        "\n",
        "# adds a dir for the dataset\n",
        "dataset = tf.keras.utils.get_file(\"stack_overflow_16k.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "# getting the full path to the directory\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiftx5OWwWWD",
        "outputId": "07ad4fe1-7366-4fb3-ee5c-914ba0df61b3"
      },
      "source": [
        "# listing all the sub paths in the dataset's ZIP folder\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'stack_overflow_16k.tar.gz.tar.gz',\n",
              " 'test',\n",
              " 'train',\n",
              " 'README.md',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGzqEtOIwgkU",
        "outputId": "1b95a5b1-be5f-4193-e46e-08b4273f74d4"
      },
      "source": [
        "# do the same just for the training data directory\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['java', 'csharp', 'javascript', 'python']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvK6Mkthw_Sv"
      },
      "source": [
        "**Example Data**:\n",
        "Let's take a look at one of the questions for Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-uVbtqvxKaz",
        "outputId": "09a1c406-08b7-4484-fab5-5cbaf617eb37"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'python/102.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"why not use self.varibale_name while calling instance attribute when we create a class which is having instance attribute,why don't we call the instance attribute using self keyword.please..class car:.    def __init__(self, type, color):.        self.type = type.        self.color = color...c1 = car('suv','red').print(c1.type)...why not print(c1.self.type), because self.type is the actual attribute..got the following error:..attributeerror: 'car' object has no attribute 'self'\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4vXEAsDxj0S"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "In this step we'll do a number of things to prepare the data for text classification including:\n",
        "\n",
        "1.   Loading the data using `tf.data.Dataset` ([docs](https://www.tensorflow.org/guide/data))\n",
        "2.   Splitting the data into training, validation, and testing sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H_-lu-Myb35"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDc-9474yeh0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}