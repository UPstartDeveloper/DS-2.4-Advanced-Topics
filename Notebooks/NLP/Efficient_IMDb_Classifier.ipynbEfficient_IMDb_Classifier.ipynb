{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Efficient_IMDb_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4uGc9+jVEwp6iwhG2BbBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPstartDeveloper/DS-2.4-Advanced-Topics/blob/main/Notebooks/NLP/Efficient_IMDb_Classifier.ipynbEfficient_IMDb_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckuh-aSbJ1C"
      },
      "source": [
        "# Exploring the Data API\n",
        "\n",
        "In this exercise we'll revist the IMDb dataset, but this time we'll use the features of the Tensorflow Data API, `tf.data`, to implement highly performant input pipelines.\n",
        "\n",
        "We'll also take another look at making language models for binary classification, and use an `Embedding` layer to see if we can get a computer to learn the implicit relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cdbEI04bjME"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCSJxoLblj8"
      },
      "source": [
        "# Copied from Aurélien Géron's Ch. 13 notebook, \n",
        "# for \"Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow\": \n",
        "# https://colab.research.google.com/github/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb\n",
        "\n",
        "\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To Download the Dataset (see Part 1)\n",
        "from pathlib import Path\n",
        "\n",
        "# for easily counting the frequency of items in a collection\n",
        "from collections import Counter"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myKm9V-b9Ol"
      },
      "source": [
        "## Part 1: Get the Data\n",
        "\n",
        "**About the IMDb Dataset**:\n",
        "1. 50,000 movies reviews from the Internet Movie Database (IMDb). \n",
        "2. Training and testing data are in `train/` and `test/`\n",
        "3. Both of these directories has their own subdirectories for samples of `pos/` and `neg/` reviews.\n",
        "4. Dataset is *balanced* (\n",
        "  - 12,500 samples per class, in both the training and test data\n",
        "5. The samples themselves are *text files.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4QHemPua238",
        "outputId": "4b46f464-54af-4d21-fcf8-14ab8e5a2b08"
      },
      "source": [
        "# locating the dataset TAR file\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "# downloading it onto the client machine\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, \n",
        "                                extract=True)\n",
        "# finding a place for it on our machine\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "# here it is!\n",
        "print(path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/aclImdb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcXXxxhikKcc"
      },
      "source": [
        "## Part 2: Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8x1He7jeov9"
      },
      "source": [
        "def review_paths(dirpath):\n",
        "    \"\"\"Given a directory path, returns a list of all the text files present.\n",
        "\n",
        "    Args:\n",
        "      dirpath: str. The path to a folder on the filesystem.\n",
        "\n",
        "    Returns: List[str]\n",
        "\n",
        "    Example Usage:\n",
        "    review_paths(\"/root/.foo\") ==> [\"bar.txt\", \"foobar.txt\"]\n",
        "    \"\"\"\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "\n",
        "# collect samples for each of the training data, divided by class\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "# do the same for test data (includes data we'll use for validation as well)\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaztz67QjEeE",
        "outputId": "0435399a-0323-4932-bddc-4fef18a51b93"
      },
      "source": [
        "# verify we collected all the samples for each section\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500, 12500, 12500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luFRJz1Gjo9O"
      },
      "source": [
        "# shuffle test data to make sure the validation data is identically distributed\n",
        "np.random.shuffle(test_valid_neg)\n",
        "np.random.shuffle(test_valid_pos)\n",
        "# shuffle training data so the model doesn't learn based on it is ordered\n",
        "np.random.shuffle(train_pos)\n",
        "np.random.shuffle(train_neg)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNNAvwG1hm75"
      },
      "source": [
        "To aid our training process, we'll create a separate validation set from 15,000 of the samples in the testing data \n",
        "\n",
        "The remaining 10,000 samples of the test data will be kept separate, and not seen by the model until after training is completed of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GdAUYahj-mc"
      },
      "source": [
        "# keep just 5,000 samples of pos and neg test data \n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "# the rest of the data is for validation\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yhCEUckpHh"
      },
      "source": [
        "## Part 3: Using the Data API\n",
        "\n",
        "Say hello to `tf.data`!\n",
        "\n",
        "Pretending as though this dataset were humongous, we could read it efficiently using `TextLineDataset`. This works for 2 reasons:\n",
        "\n",
        "1. Each review is 1 just line of text long\n",
        "2. If this dataset was in fact humongous, then this technique allows us to avoid loading it in all at once into RAM. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzloct-Okl_M"
      },
      "source": [
        "def convert_to_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
        "    \"\"\"Create a Tensorflow-based dataset object from the dataset on disk.\n",
        "\n",
        "    This is intended for use on binary classification problems only.\n",
        "    \n",
        "    Args:\n",
        "      filepaths_positive: str. The path of the positive (1) samples.\n",
        "      filepaths_negative: str. The path to the negative (0) samples.\n",
        "      n_read_threads: int. Specifies the number of threads to use,\n",
        "        so we can read in multiple records at once. \n",
        "\n",
        "    Returns: tf.data.Dataset: contain labeled data for both classes.\n",
        "    \"\"\"\n",
        "    # read in the negative reviews\n",
        "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    # read in the positive reviews\n",
        "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    # label the positive and negative reviews numerically \n",
        "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
        "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
        "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jTB4AL4qAex"
      },
      "source": [
        "Speed test!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDli1xqqqslq",
        "outputId": "2b27803c-6252-4f6c-df31-1e63350501f7"
      },
      "source": [
        "%timeit -r1 convert_to_dataset(train_pos, train_neg).repeat(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 1: 40.9 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "294ty3aUqu7Z"
      },
      "source": [
        "Oh, not fast enough? We can make it more efficient by using `.cache()`!\n",
        "\n",
        "Again, if the dataset is small enough we could also just make the tensor of the dataset (aka `tf.data.Dataset`) by just loading it into memory, and it would be just as efficient as the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Hnjc5jqAEA",
        "outputId": "c26d599e-2eaa-4da8-f38a-e4cf60aaca27"
      },
      "source": [
        "%timeit -r1 convert_to_dataset(train_pos, train_neg).cache().repeat(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 1: 40.7 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrsOl2zkrnL4"
      },
      "source": [
        "Let's do this for all of our data splits. \n",
        "\n",
        "We'll add some extra efficiency by using `prefetch()`. This tells our hardware to simultaneously grab batches for training, as the previous batch is currently being processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGaMBHk3qH1i"
      },
      "source": [
        "batch_size = 32\n",
        "# Optionally, we have shuffled the training data once more\n",
        "train_set = convert_to_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
        "valid_set = convert_to_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
        "test_set = convert_to_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1J4PVe_tCux"
      },
      "source": [
        "## Part 4: Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4SFbLIt2hP"
      },
      "source": [
        "### Text Preprocessing (Layers + Operations)\n",
        "\n",
        "This comes first, as neural networks cannot understand raw text!\n",
        "\n",
        "Since we will frequently need to refer to the maximum number of tokens we believe are in the dataset, we'll start this section by saving a constant for this value (I arbitrarily chose 5000):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux2GvVeb2DJm"
      },
      "source": [
        "MAX_VOCAB_SIZE = 5000  "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0YjmYK42G9-"
      },
      "source": [
        "Next we'll need a helper function for cleaning the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tnxYNDrs7OR"
      },
      "source": [
        "def preprocess(X_batch, n_words=50):\n",
        "    \"\"\"Preprocess the input text for a single batch of data.\n",
        "\n",
        "    Args:\n",
        "      X_batch: tf.data.Dataset. A tensor of several text samples\n",
        "      n_words: int. The desired number of tokens for each review\n",
        "\n",
        "    Returns: tf.Tensor. Cleaned text version of the batch.\n",
        "    \"\"\"\n",
        "    # store the shape of the batch as a tensor\n",
        "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
        "    # crop the review lengths to 300 characters\n",
        "    Z = tf.strings.substr(X_batch, 0, 300)\n",
        "    # lower-case the reviews\n",
        "    Z = tf.strings.lower(Z)\n",
        "    # text-cleaning: replace all <br /> and all non-letter characters w/ spaces\n",
        "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
        "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
        "    # tokenize the review\n",
        "    Z = tf.strings.split(Z)\n",
        "    # ensure each review has n_words token (padding or cropping)\n",
        "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8zwwSJK1pJz"
      },
      "source": [
        "This utility will ensure the tokens are encoded with the **most frequent tokens getting\n",
        " the lowest indices**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGHw8OL81sEE"
      },
      "source": [
        "def get_vocabulary(data_sample, max_size=MAX_VOCAB_SIZE):\n",
        "    \"\"\"List unique tokens in a corpus, sorted by most frequently occurring.\n",
        "\n",
        "    The tokens will be listed as byte strings, as that works best with TF\n",
        "\n",
        "    Args:\n",
        "      data_sample: tf.Tensor. Contains the input strings.\n",
        "      max_size: int. The maximum number of tokens we believe are in the dataset.\n",
        "\n",
        "    Returns: List[str].\n",
        "    \"\"\"\n",
        "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
        "    # counter the frequencies of different tokens\n",
        "    counter = Counter()\n",
        "    for words in preprocessed_reviews:\n",
        "        for word in words:\n",
        "            if word != b\"<pad>\":\n",
        "                counter[word] += 1\n",
        "    # sort the tokens by occurence, make sure the padding token appears first\n",
        "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Ffqxs0zYXk"
      },
      "source": [
        "Now we will define our `TextVectorization` layer (Keras also has its own version of this, however it is currently experimental):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm3NGGGQvKoX"
      },
      "source": [
        "class TextVectorization(keras.layers.Layer):\n",
        "    def __init__(self, max_vocabulary_size=MAX_VOCAB_SIZE, n_oov_buckets=100, \n",
        "                dtype=tf.string, **kwargs):\n",
        "        '''define the hyperparams for our text corpus'''\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.max_vocabulary_size = max_vocabulary_size\n",
        "        self.n_oov_buckets = n_oov_buckets\n",
        "\n",
        "    def adapt(self, data_sample):\n",
        "        '''create words IDs for the vocabulary of our text corpus'''\n",
        "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
        "        words = tf.constant(self.vocab)\n",
        "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
        "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        '''convert input text to the corresponding integers in the vocabulary'''\n",
        "        preprocessed_inputs = preprocess(inputs)\n",
        "        return self.table.lookup(preprocessed_inputs)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DgLKve0dA8"
      },
      "source": [
        "Now we can apply this layer to our dataset (if it were humongous, we could also just use a representative sample of the dataset's vocabulary):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz1DzY0F0W1f"
      },
      "source": [
        "# A: define the max number of unique tokens that we expect are in the corpus\n",
        "max_vocabulary_size = MAX_VOCAB_SIZE\n",
        "n_oov_buckets = 100\n",
        "\n",
        "# B: store a sample of JUST the input text as an iterable\n",
        "sample_review_batches = train_set.map(lambda review, label: review)\n",
        "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
        "                                axis=0)\n",
        "# C: now just apply the vectorizer, to encode the word ids \n",
        "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
        "                                       input_shape=[])\n",
        "text_vectorization.adapt(sample_reviews)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiHIjxzi4X9B"
      },
      "source": [
        "As the very last step in preprocessing, we need to encode the text samples themselves into a numerical format - for example, we can use a custom layer for the bag-of-words approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gznvv0OF1OrS"
      },
      "source": [
        "class BagOfWords(keras.layers.Layer):\n",
        "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
        "        '''n_tokens tells us the length that the bag of word vectors must be'''\n",
        "        super().__init__(dtype=tf.int32, **kwargs)\n",
        "        self.n_tokens = n_tokens\n",
        "    def call(self, inputs):\n",
        "        '''creating the bag of word vectors on the input'''\n",
        "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
        "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5RzUIgF5GdX"
      },
      "source": [
        "The next step is to adapt the bag of words layer to our dataset of course:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8qx_xB43Zn"
      },
      "source": [
        " # store size of the vocabulary (including an extra for the <pad> token)\n",
        "n_tokens = max_vocabulary_size + n_oov_buckets + 1 \n",
        "# use the BoW layer\n",
        "bag_of_words = BagOfWords(n_tokens)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtUz5yB45Uz6"
      },
      "source": [
        "## Build + Train the Model\n",
        "\n",
        "Here we will use a fully-connected network, that has:\n",
        "\n",
        "- a text vectorization and BoW layer to do the preprocessing,\n",
        "- 1 hidden layer with 100 neurons, and ReLU activation,\n",
        "- 5 epochs of training,\n",
        "- 1 neuron and sigmoid activation in the output layer (since it is binary classification)\n",
        "- Nadam optimization\n",
        "\n",
        "And we will track accuracy as the metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72ZxenhE5S_Y",
        "outputId": "2196f4b3-e671-4822-ed57-55ea08e5fb28"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    text_vectorization,\n",
        "    bag_of_words,\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 38s 40ms/step - loss: 0.5532 - accuracy: 0.7057 - val_loss: 0.5111 - val_accuracy: 0.7457\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 33s 39ms/step - loss: 0.3702 - accuracy: 0.8318 - val_loss: 0.4947 - val_accuracy: 0.7629\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 33s 39ms/step - loss: 0.2683 - accuracy: 0.8951 - val_loss: 0.5397 - val_accuracy: 0.7592\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 33s 39ms/step - loss: 0.1491 - accuracy: 0.9546 - val_loss: 0.6342 - val_accuracy: 0.7473\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 33s 39ms/step - loss: 0.0648 - accuracy: 0.9903 - val_loss: 0.7549 - val_accuracy: 0.7487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2a34e5e0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdNdTJY6G7P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}