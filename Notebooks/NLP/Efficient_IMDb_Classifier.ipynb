{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Efficient_IMDb_Classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0XIefqv0+Eb0G9xxkiGNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UPstartDeveloper/DS-2.4-Advanced-Topics/blob/main/Notebooks/NLP/Efficient_IMDb_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckuh-aSbJ1C"
      },
      "source": [
        "# Exploring the Data API\n",
        "\n",
        "In this exercise we'll revist the IMDb dataset, but this time we'll use the features of the Tensorflow Data API, `tf.data`, to implement highly performant input pipelines.\n",
        "\n",
        "We'll also take another look at making language models for binary classification, and use an `Embedding` layer to see if we can get a computer to learn the implicit relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cdbEI04bjME"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCSJxoLblj8"
      },
      "source": [
        "# Copied from Aurélien Géron's Ch. 13 notebook, \n",
        "# for \"Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow\": \n",
        "# https://colab.research.google.com/github/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb\n",
        "\n",
        "\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To Download the Dataset (see Part 1)\n",
        "from pathlib import Path"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myKm9V-b9Ol"
      },
      "source": [
        "## Part 1: Get the Data\n",
        "\n",
        "**About the IMDb Dataset**:\n",
        "1. 50,000 movies reviews from the Internet Movie Database (IMDb). \n",
        "2. Training and testing data are in `train/` and `test/`\n",
        "3. Both of these directories has their own subdirectories for samples of `pos/` and `neg/` reviews.\n",
        "4. Dataset is *balanced* (\n",
        "  - 12,500 samples per class, in both the training and test data\n",
        "5. The samples themselves are *text files.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4QHemPua238",
        "outputId": "4b46f464-54af-4d21-fcf8-14ab8e5a2b08"
      },
      "source": [
        "# locating the dataset TAR file\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "# downloading it onto the client machine\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, \n",
        "                                extract=True)\n",
        "# finding a place for it on our machine\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "# here it is!\n",
        "print(path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/aclImdb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcXXxxhikKcc"
      },
      "source": [
        "## Part 2: Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8x1He7jeov9"
      },
      "source": [
        "def review_paths(dirpath):\n",
        "    \"\"\"Given a directory path, returns a list of all the text files present.\n",
        "\n",
        "    Args:\n",
        "      dirpath: str. The path to a folder on the filesystem.\n",
        "\n",
        "    Returns: List[str]\n",
        "\n",
        "    Example Usage:\n",
        "    review_paths(\"/root/.foo\") ==> [\"bar.txt\", \"foobar.txt\"]\n",
        "    \"\"\"\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "\n",
        "# collect samples for each of the training data, divided by class\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "# do the same for test data (includes data we'll use for validation as well)\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaztz67QjEeE",
        "outputId": "0435399a-0323-4932-bddc-4fef18a51b93"
      },
      "source": [
        "# verify we collected all the samples for each section\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500, 12500, 12500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luFRJz1Gjo9O"
      },
      "source": [
        "# shuffle all the data for good measure\n",
        "np.random.shuffle(test_valid_pos)\n",
        "np.random.shuffle(train_pos)\n",
        "np.random.shuffle(test_valid_neg)\n",
        "np.random.shuffle(train_neg)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNNAvwG1hm75"
      },
      "source": [
        "To aid our training process, we'll create a separate validation set from 15,000 of the samples in the testing data \n",
        "\n",
        "The remaining 10,000 samples of the test data will be kept separate, and not seen by the model until after training is completed of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GdAUYahj-mc"
      },
      "source": [
        "# keep just 5,000 samples of pos and neg test data \n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "# the rest of the data is for validation\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yhCEUckpHh"
      },
      "source": [
        "## Part 3: Using the Data API\n",
        "\n",
        "Say hello to `tf.data`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzloct-Okl_M"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}